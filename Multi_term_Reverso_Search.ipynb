{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNki5r3NOSrWDakS2FT4xMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasetrad/Search-Engine/blob/main/Multi_term_Reverso_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google.colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx0e7zTvwuCa",
        "outputId": "30306140-7622-4766-83d4-259ec51a294f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google.colab in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: tornado~=5.1.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (5.1.1)\n",
            "Requirement already satisfied: notebook~=5.5.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (5.5.0)\n",
            "Requirement already satisfied: google-auth>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from google.colab) (1.35.0)\n",
            "Requirement already satisfied: astor~=0.8.1 in /usr/local/lib/python3.7/dist-packages (from google.colab) (0.8.1)\n",
            "Requirement already satisfied: ipython~=7.9.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (7.9.0)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (2.23.0)\n",
            "Requirement already satisfied: ipykernel~=5.3.4 in /usr/local/lib/python3.7/dist-packages (from google.colab) (5.3.4)\n",
            "Requirement already satisfied: portpicker~=1.3.1 in /usr/local/lib/python3.7/dist-packages (from google.colab) (1.3.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (4.2.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel~=5.3.4->google.colab) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel~=5.3.4->google.colab) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython~=7.9.0->google.colab) (0.8.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (23.2.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (5.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (2.11.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (4.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (0.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel~=5.3.4->google.colab) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->google.colab) (2022.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->google.colab) (1.21.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython~=7.9.0->google.colab) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.17.2->google.colab) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (2.10)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook~=5.5.0->google.colab) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook~=5.5.0->google.colab) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (5.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.4)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook~=5.5.0->google.colab) (4.13.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook~=5.5.0->google.colab) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook~=5.5.0->google.colab) (2.16.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook~=5.5.0->google.colab) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook~=5.5.0->google.colab) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.5.0->google.colab) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.5.0->google.colab) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.5.0->google.colab) (22.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook~=5.5.0->google.colab) (0.5.1)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de archivos html para diccionarios #"
      ],
      "metadata": {
        "id": "G_kRu0D6uvtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "xDmoj8uEutX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvfZH_h54cBM",
        "outputId": "def17f04-d46e-4501-8fa1-26b91a7dc7f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qNm31uNGzjH",
        "outputId": "bc187641-022c-4e3c-c76f-5a582bd7d401"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"# BusquedasMasivas\" >> README.md\n",
        "!git init\n",
        "!git add README.md\n",
        "!git commit -m \"first commit\"\n",
        "!git branch -M main\n",
        "!git remote add origin https://github.com/pasetrad/Search-Engine.git\n",
        "!git push -u origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VC0WSEjHPzg",
        "outputId": "ea638325-8c91-451e-babb-47a75901fd14"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/drive/MyDrive/Github/.git/\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@5a806468de7b.(none)')\n",
            "error: refname refs/heads/master not found\n",
            "fatal: Branch rename failed\n",
            "error: src refspec main does not match any.\n",
            "error: failed to push some refs to 'https://github.com/pasetrad/Search-Engine.git'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"pasetrad\"\n",
        "with open('/content/drive/MyDrive/Ponencia-Tecnología traduccion-texto referencia.txt') as f:\n",
        "    lines = f.readline()\n",
        "repository = \"BusquedasMasivas\" #https://github.com/pasetrad/.git\n",
        "git_token = lines\n",
        "!git clone https://{git_token}@github.com/{username}/{repository}.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyRbAbaV9HBZ",
        "outputId": "1c7e5cfc-7cef-4f0f-84e9-20ce12913b4c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BusquedasMasivas'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importación de librerías y definición de método (Reverso_Search)"
      ],
      "metadata": {
        "id": "cZwCADeL2ksJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REVERSO ###"
      ],
      "metadata": {
        "id": "KHV5gFTieYr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def Reverso_Search(data):\n",
        "  headers = {\n",
        "      \"Connection\": \"keep-alive\",\n",
        "      \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
        "      \"X-Requested-With\": \"XMLHttpRequest\",\n",
        "      \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\",\n",
        "      \"Content-Type\": \"application/json; charset=UTF-8\",\n",
        "      \"Content-Length\": \"96\",\n",
        "      \"Origin\": \"https://context.reverso.net\",\n",
        "      \"Sec-Fetch-Site\": \"same-origin\",\n",
        "      \"Sec-Fetch-Mode\": \"cors\",\n",
        "      \"Referer\": \"https://context.reverso.net/^%^D0^%^BF^%^D0^%^B5^%^D1^%^80^%^D0^%^B5^%^D0^%^B2^%^D0^%^BE^%^D0^%^B4/^%^D0^%^B0^%^D0^%^BD^%^D0^%^B3^%^D0^%^BB^%^D0^%^B8^%^D0^%^B9^%^D1^%^81^%^D0^%^BA^%^D0^%^B8^%^D0^%^B9-^%^D1^%^80^%^D1^%^83^%^D1^%^81^%^D1^%^81^%^D0^%^BA^%^D0^%^B8^%^D0^%^B9/cat\",\n",
        "      \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "      \"Accept-Language\": \"es-ES,es;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "  }\n",
        "  #Acá abajo hay que pegar la lista de elementos. Para hacerlo, debes hacer lo siguiente:\n",
        "  #Crear lista en Excel y poner ruta del archivo abajo cambiando sheetname y la ruta del archivo. \n",
        "  search_terms = pd.DataFrame(data, columns= ['Inglés'])\n",
        "  list = search_terms['Inglés'].tolist()\n",
        "  print(list)\n",
        "  #list = [\"Flood of departures\" , \"Cite\" , \"Work-life balance\" , \"Findings\" , \"Department of Health and Social Care\" , \"Engage Britain\" , \"Public conciousness\" , \"Has long had significantly lower numbers\" , \"Around 50 in every 10\" , \"000 staff \" , \"hospital and community health services \" , \"within the next three months\" , \"Analysis of NHS Digital figures found that at least 400 staff a week \" , \"comes alongside evidence\" , \"high turnover among social care workers\" , \"left their roles\" , \"Commons health select committee\" , \"Care work and nursing\" , \"Undervalued professions\" , \"senior occupational therapist\" , \"were named as some of our most undervalued professions\" , \" with 69% saying\" , \"health and social care services\" , \"mental health issues \" , \"preventative healthcare\" , \"It wasn’t a difficult decision to go private\" , \"overworked and undervalued \" , \"recruitment and retention of staff\" , \"under-resourced teams\" , \"massive\" , \"Ultimately\" , \" \" , \"hiatus hernia\" , \"ward for dementia patients\" , \"bedpan\" , \"painkillers\" , \"overstretched\" , \"spiralling downwards\" , \"deliver health and care\"]\n",
        "  output = pd.DataFrame([],columns=['Inglés', 'Español'])\n",
        "  \n",
        "  for i in list:\n",
        "    data = {\n",
        "      \"source_text\": i,\n",
        "      \"target_text\": \"\",\n",
        "      \"source_lang\": \"en\",\n",
        "      \"target_lang\": \"es\",\n",
        "      \"npage\": 1,\n",
        "      \"mode\": 0\n",
        "    }\n",
        "\n",
        "    npages = requests.post(\"https://context.reverso.net/bst-query-service\", headers=headers, data=json.dumps(data)).json()[\"npages\"]\n",
        "    for npage in range(1, 10):\n",
        "        data[\"npage\"] = npage\n",
        "        page = requests.post(\"https://context.reverso.net/bst-query-service\", headers=headers, data=json.dumps(data)).json()[\"list\"]\n",
        "        for word in page:\n",
        "            output = output.append({'Inglés': BeautifulSoup(word[\"s_text\"]).text,'Español': BeautifulSoup(word[\"t_text\"]).text},ignore_index = True)\n",
        "            print(output)\n",
        "  return output"
      ],
      "metadata": {
        "id": "sDwyB-I71PsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yI3ulihIq6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Búsqueda masiva de tr-ex.me"
      ],
      "metadata": {
        "id": "wohT2Xq69rEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = [\"Flood of departures\" , \"Cite\" , \"Work-life balance\" , \"Findings\" , \"Department of Health and Social Care\" , \"Engage Britain\" , \"Public conciousness\" , \"Has long had significantly lower numbers\" , \"Around 50 in every 10\" , \"000 staff \" , \"hospital and community health services \" , \"within the next three months\" , \"Analysis of NHS Digital figures found that at least 400 staff a week \" , \"comes alongside evidence\" , \"high turnover among social care workers\" , \"left their roles\" , \"Commons health select committee\" , \"Care work and nursing\" , \"Undervalued professions\" , \"senior occupational therapist\" , \"were named as some of our most undervalued professions\" , \" with 69% saying\" , \"health and social care services\" , \"mental health issues \" , \"preventative healthcare\" , \"It wasn’t a difficult decision to go private\" , \"overworked and undervalued \" , \"recruitment and retention of staff\" , \"under-resourced teams\" , \"massive\" , \"Ultimately\" , \" \" , \"hiatus hernia\" , \"ward for dementia patients\" , \"bedpan\" , \"painkillers\" , \"overstretched\" , \"spiralling downwards\" , \"deliver health and care\"]\n",
        "output = pd.DataFrame([],columns=['Inglés', 'Español'])\n",
        "list1 = [\"Flood of departures\" , \"Cite\"] \n",
        "  for i in list1:\n",
        "    data = {\n",
        "      \"source_text\": i,\n",
        "      \"target_text\": \"\",\n",
        "      \"source_lang\": \"en\",\n",
        "      \"target_lang\": \"es\",\n",
        "      \"npage\": 1,\n",
        "      \"mode\": 0\n",
        "     \"mode\": 0\n",
        "    }\n",
        "    source = \"english\"\n",
        "    target = \"spanish\"\n",
        "    npages = requests.post(\"https://tr-ex.me/translation/\"+source+\"-\"+target+\"\\\"\\\"\", headers=headers, data=json.dumps(data)).json()[\"npages\"]\n",
        "    for npage in range(1, 10):\n",
        "        data[\"npage\"] = npage\n",
        "        page = requests.post(\"https://context.reverso.net/bst-query-service\", headers=headers, data=json.dumps(data)).json()[\"list\"]\n",
        "        for word in page:\n",
        "            output = output.append({'Inglés': BeautifulSoup(word[\"s_text\"]).text,'Español': BeautifulSoup(word[\"t_text\"]).text},ignore_index = True)\n",
        "            print(output)\n",
        "  return output\n",
        "      "
      ],
      "metadata": {
        "id": "Iqm8NyRV9qEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db92ad7-a7b6-42d0-e32e-2e9cb7e0fda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-6f33215f5e49>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for i in list1:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdl7d4WUHmmT",
        "outputId": "cf63eb21-d759-40c2-9a2c-1c0d48f898e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vih1TwxIb-7b",
        "outputId": "b77072b1-45ef-4ee6-ed7a-732ccfcfd9bb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.7.0-py2.py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=21.0.0\n",
            "  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting itemloaders>=1.0.1\n",
            "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n",
            "Collecting Twisted>=18.9.0\n",
            "  Downloading Twisted-22.8.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 43.0 MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n",
            "Collecting cryptography>=3.3\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting zope.interface>=5.1.0\n",
            "  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n",
            "Collecting PyDispatcher>=2.0.5\n",
            "  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0\n",
            "  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting parsel>=1.5.0\n",
            "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n",
            "Collecting jmespath>=0.9.5\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n",
            "Collecting incremental>=21.3.0\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n",
            "Collecting Automat>=0.8.0\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=6464e53a1bfe3c7e0f4f59eab04f06964467b22bb88e6e21ea916aee334d5831\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Twisted-22.8.0 constantly-15.1.0 cryptography-38.0.1 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.6.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.0 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.0.1 zope.interface-5.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
        "list1 = [\"Flood of departures\" , \"Cite\"] \n",
        "print(list1[0])\n",
        "for i in range(0, len(list1)):\n",
        "  tokens = wordpunct_tokenize(list1[i])\n",
        "  token = '+'.join(tokens)\n",
        "  print(token)\n",
        "item1 = re.split(r'\\b\\+', list1[0])\n",
        "print(item1)\n",
        "\n",
        "#https://tr-ex.me/translation/english-spanish/%22flood+of+departures%22?search=%22flood+of+departures%22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8txZ7wJB020",
        "outputId": "918c2e13-2ca1-4b53-cffc-4eef735d73a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flood of departures\n",
            "Flood+of+departures\n",
            "Cite\n",
            "['Flood of departures']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código extraido de Github"
      ],
      "metadata": {
        "id": "QZNIuLka6gaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LINGUEE ###"
      ],
      "metadata": {
        "id": "aobSlLsY2-4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente código sirve para extraer segmentos o unidades de traducción que contengan la palabra o las frases que buscamos en Linguee."
      ],
      "metadata": {
        "id": "mCswLVeXDnM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import files\n",
        "def LingueeSearch_EN_ES(term):\n",
        "  api_root = \"https://linguee-api.fly.dev/api/v2\"\n",
        "  resp = requests.get(f\"{api_root}/external_sources\", params={\"query\": term, \"src\": \"en\", \"dst\": \"es\"})\n",
        "  output = pd.DataFrame([],columns=['Inglés', 'Español'])\n",
        "  for source in resp.json():\n",
        "    print(f\"{source['src']} -> {source['dst']}\")\n",
        "    output = output.append({'Inglés': BeautifulSoup(source['src']).text,'Español': BeautifulSoup(source['dst']).text},ignore_index = True)\n",
        "  return output"
      ],
      "metadata": {
        "id": "y1hOCiM-3Hja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term = input(\"Ingrese su término o frase: \")\n",
        "output = LingueeSearch_EN_ES(term)\n",
        "archivo = output.to_excel(r'/content/Segmentos-enes.xlsx', index = False)\n",
        "#print(output)\n",
        "files.download('/content/Segmentos-enes.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f4E43UNzC5u",
        "outputId": "86c32aef-3bda-4e3b-e93b-70024a89865b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese su término o frase: basket press\n",
            "A basket press in which pressure is applied by a large central screw. -> Aparato empleado para rebajar la temperatura del mosto durante su fermentacion.\n",
            "A basket press in which pressure is applied by a large central screw. -> Aparato empleado para separar el escobajo o raspon de los granos de uva.\n",
            "The press [...] used is a classical basket press. -> Prensado en una prensa vertical.\n",
            "At first, they made wine on an old [...] parking lot at the rancho, with a small mill, a manual [...] corking machine, a little basket press, and 20 American oak [...] barrels. -> A principio, hacían vino en un antiguo parqueadero [...] en el rancho, con un pequeño molino, una maquina [...] encorchadora manual, una pequeña prensa manual y 20 barriles [...] de roble.\n",
            "The Power Light will come on. If the unit is equipped with the Basket Elevator Option, and [...] you want to engage this option, press the Basket Elevator Switch to the ON position. -> La luz verde se encenderá Si la unidad esta equipada [...] con la opción del Elevador de la Canasta y usted desea activar [...] esta opción, coloque el Interruptor del Elevador de la Canasta en la posición [...] de encendido \"ON\" .\n",
            "Pasta 2001 is totally automatic and can make significant savings in employee time: all that's [...] needed is to load the basket and press a button. -> Pasta 2001' es una máquina cuecepasta totalmente [...] automática que permite reducir considerablemente el personal [...] necesario: basta cargar la cesta y pulsar el botón de [...] encendido.\n",
            "After preliminary mashing, the apples [...] are placed in a woven basket between the jaws of the press that is then screwed down [...] to produce the juice. -> Las manzanas son apaleadas en una [...] gran batea o dornajo de madera, [...] luego se colocan en canasto flexible y aplastada en esta prensa para que escurra [...] su jugo.\n",
            "as basket width and press the OK key. -> y apretar la tecla \"OK\".\n",
            "The Coupon code box is [...] displayed on the first page of the shopping basket: after you have inserted your items in the basket and entered your code, press \"Apply\". -> El recuadro para introducir el código se [...] muestra en la primera [...] página de la cesta de la compra: después de haber seleccionado los artículos que desea e introducir su código, haga clic en \"Aplicar\".\n",
            "To cancel one item [...] type 0 in quantity and press the Refresh Basket button. -> Para anular una [...] línea ponga las unidades a 0 y pulse el botón de actualizar.\n",
            "Press the desired Basket Timer again to set or or -> Presione el botón de la canasta deseada nuevamente para [...] programar el tiempo. ó ó\n",
            "If you need more than one unit [...] indicate in the box and press \"Update basket\". -> Si quiere mas de una unidad indiquelo en la casilla [...] correspondiente y pulse en \"Actualizar cesta\".\n",
            "Press the desired Basket Timer. or -> Presione el botón del tiempo para la canasta deseada ó\n",
            "Since this is, of course, not in keeping with the politically correct image that we have, this report's conclusions were confined to the waste paper basket. -> Dado que, por supuesto, esto no concuerda con la imagen políticamente correcta que tenemos, las conclusiones de ese estudio se archivaron en la papelera.\n",
            "Invite visitors to participate in your survey depending on their behavior: Are you [...] interested in only those visitors who spend 5 minutes on your website, [...] fill their shopping basket but then don't make [...] a purchase? -> Invite a los participantes en la encuesta en función de su comportamiento en el sitio web: ¿Le [...] interesan únicamente los visitantes que permanecen 5 minutos en el [...] sitio web, llenan la cesta, pero al final no compran nada?\n",
            "The odd dog, of the sort that they [...] call around here \"basket dogs\", because they [...] have the job of guarding the food bag while [...] the master works, follows along friskily and absent-mindedly. -> Algún perrillo, de aquellos que [...] aquí llaman \"perros de cesto\" porque tienen el [...] cometido de vigilar el capazo de la comida [...] mientras el amo trabaja, sigue amedrentado y despistado.\n",
            "The two baskets inside can clip together to form a dishwasher basket, which makes [...] pre-cleaning of smaller items like soothers and nipples much easier. -> Las dos cestas interiores se adaptan formando una cesta para el lavavajillas, lo que [...] hace que la tarea de limpiar los pequeños [...] artículos como chupetes y tetinas sea mucho más fácil.\n",
            "While the result was a chair that looked as [...] much like a laundry basket as a chair, the design [...] was highly functional and gave rise [...] to a cottage industry that continues today. -> Aunque el resultado fue una silla [...] que parecía más un cesto para ropa, el diseño [...] fue muy funcional y dio origen a una industria [...] artesanal que persiste hasta nuestros días.\n",
            "If you try any other approach, your complaints end up in the wastepaper basket. -> Si se opta por cualquier otro enfoque, las reclamaciones terminan en la papelera.\n",
            "On its own, that staff dispute would not have created [...] much of a stir in the press. -> En sí misma, esa disputa del personal no habría causado [...] mucho revuelo en la prensa.\n",
            "This debate has raised in particular the issue of [...] the freedom of the press in Tunisia. -> Este debate ha suscitado concretamente el tema [...] de la libertad de prensa en Túnez.\n",
            "(24) To distinguish the new product [...] type from ready to press powders within CN [...] code 3824 30 00, the following criteria can [...] be applied: macroscopic aspect, particle size, and chemical composition and flow properties. -> (24) Para distinguir el nuevo tipo de producto de los [...] polvos listos para prensar dentro del código [...] NC 3824 30 00, pueden aplicarse los siguientes [...] criterios: aspecto macroscópico, tamaño de las partículas, composición química y propiedades de flujo.\n",
            "At the same time he worked as an independent [...] journalist for the news [...] agency Havana Press, one of the ten news agencies belonging to the Nueva Prensa Cubana press group. -> Paralelamente trabajaba como periodista [...] independiente para la [...] agencia Habana Press, una de las diez agencias de prensa que pertenecen al grupo Nueva Prensa Cubana.\n",
            "See statement by Assistant Secretary Juan Zarate before the Al-Qaida and Taliban Sanctions Committee, available [...] at www.treas.gov/press/releases/js2189.htm. -> Véase la declaración del Subsecretario Juan [...] Zárate ante el Comité de Sanciones contra Al-Qaida y los talibanes, disponible [...] en www.treas.gov/press/releases/js2189.htm.\n",
            "Du Bouchet, who is a [...] reporter for Habana Press, an independent news [...] agency, was previously jailed from August 2005 to August 2006 for a similar reason. -> El corresponsal habanero de la agencia [...] independiente Habana Press ya cumplió condena [...] de un año de cárcel por un motivo similar, [...] desde agosto de 2005 a agosto de 2006.\n",
            "Moreover, such an interpretation of culture requires a critical [...] assessment of the impact and effects that decisions in the first [...] and second policy baskets might have on the third basket. -> Además, esta interpretación de la cultura requiere una evaluación crítica de [...] las consecuencias que toda decisión adoptada con respecto a los [...] dos primeros pilares podría tener sobre el tercer pilar.\n",
            "In 2006 he published an exhaustive book about the clarinet with [...] the Yale University Press, part of a collection [...] devoted to orchestra instruments. -> En 2006 publicó un exhaustivo libro sobre el clarinete editado por la [...] Yale University Press, que forma parte de una [...] colección dedicada a los instrumentos de la orquesta.\n",
            "Women now feel \"more protected to report,\" she told [...] news bureau Inter Press Service. -> Las mujeres ahora se sienten \"más protegidas al reportar,\" [...] informó a Inter Press Service.\n",
            "The Romanian athlete Balas won in the high jump and [...] the Soviet athlete Press was the winner in [...] the discus throw (she also came in third [...] in the shot put), thereby opening an essential era for the evolution of this sporting event. -> La rumana Balas consiguió la victoria en salto de [...] altura y la soviética Press fue la campeona en [...] lanzamiento de disco (fue, además, tercera [...] en peso), abriendo una época imprescindible para conocer la evolución de este deporte.\n",
            "You'll love how quickly this elegant and rich dessert sandwich can be [...] made. Usually, it's [...] important to press grilled cheese sandwiches with a spatula to flatten them, but with this one, it's important not to press it, or the filling [...] can squirt out. -> Normalmente, es importante prensar los sándwiches de queso a la [...] plancha con una espátula para achatarlos; pero, en este caso particular, es importante no hacerlo, para evitar que se derrame el relleno.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d80a4a92-86ba-4179-83f6-1d8e95142316\", \"Segmentos-enes.xlsx\", 9449)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "\n",
        "CODES = {\n",
        "    'es': 'spanish',\n",
        "    'en': 'english'\n",
        "}\n",
        "\n",
        "\n",
        "URL_BASE = \"https://www.linguee.com/%s-%s/translation/%s.html\"\n",
        "\n",
        "# these urls can come from another data file\n",
        "def get_data_from_file(filepath: string):\n",
        "with open('data.json', 'r') as f:\n",
        "     lines = f.readlines()\n",
        "\n",
        "return [URL_BASE % (CODES['es'], CODES['en'], line) for line in lines]\n",
        "URLS = [\n",
        "    URL_BASE % (CODES['es'], CODES['en'], 'lascive')\n",
        "]\n",
        "\n",
        "\n",
        "class BlogSpider(scrapy.Spider):\n",
        "    name = 'linguee_spider'\n",
        "\n",
        "    start_urls = URLS\n",
        "\n",
        "    def parse(self, response):\n",
        "        for span in response.css('span.tag_lemma'):\n",
        "            yield {'world': span.css('a.dictLink ::text').get()}\n",
        "\n",
        "        for div in response.css('div.translation'):\n",
        "            for span in div.css('span.tag_trans'):\n",
        "                yield {'translation': span.css('a.dictLink ::text').get()}"
      ],
      "metadata": {
        "id": "F7V7vStz6kPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import requests\n",
        "import urllib\n",
        "url = \"https://www.linguee.com/english-spanish/search?source=auto&query=\"+keyword\n",
        "'''print(url)\n",
        "def getHTMLdocument(url):\n",
        "      \n",
        "    # request for HTML document of given url\n",
        "    response = requests.get(url)\n",
        "      \n",
        "    # response will be provided in JSON format\n",
        "    return response.text\n",
        "html =getHTMLdocument(url)\n",
        "# create soup object\n",
        "soup = BeautifulSoup(html,'html.parser')\n",
        "print(soup.find_all(\"result_table\"))\n",
        "#for text in soup.find_all('a','''\n",
        "\n",
        "with open(\"preisliste.csv\", \"r\", newline=\"\") as f_input:\n",
        "    csv_reader = csv.reader(f_input, delimiter=\";\", quotechar=\"|\")\n",
        "    header = next(csv_reader)\n",
        "    items = [row[1] for row in csv_reader]\n",
        "\n",
        "with open(\"results.csv\", \"w\", newline=\"\") as f_output:\n",
        "    csv_writer = csv.writer(f_output, delimiter=\";\")\n",
        "\n",
        "    for item in items: \n",
        "        search_url =\"https://www.linguee.com/english-spanish/search?source=auto&query=\".format(urllib.parse.quote_plus(item, safe='/'))"
      ],
      "metadata": {
        "id": "70ZmzzEQymh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jrh8wG94Deqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "\n",
        "def LingueeSearch_EN_ES(data):\n",
        "  search_terms = pd.DataFrame(data, columns= ['Inglés'])\n",
        "  list = search_terms['Inglés'].tolist()\n",
        "  print(list)\n",
        "  #list = [\"Flood of departures\" , \"Cite\" , \"Work-life balance\" , \"Findings\" , \"Department of Health and Social Care\" , \"Engage Britain\" , \"Public conciousness\" , \"Has long had significantly lower numbers\" , \"Around 50 in every 10\" , \"000 staff \" , \"hospital and community health services \" , \"within the next three months\" , \"Analysis of NHS Digital figures found that at least 400 staff a week \" , \"comes alongside evidence\" , \"high turnover among social care workers\" , \"left their roles\" , \"Commons health select committee\" , \"Care work and nursing\" , \"Undervalued professions\" , \"senior occupational therapist\" , \"were named as some of our most undervalued professions\" , \" with 69% saying\" , \"health and social care services\" , \"mental health issues \" , \"preventative healthcare\" , \"It wasn’t a difficult decision to go private\" , \"overworked and undervalued \" , \"recruitment and retention of staff\" , \"under-resourced teams\" , \"massive\" , \"Ultimately\" , \" \" , \"hiatus hernia\" , \"ward for dementia patients\" , \"bedpan\" , \"painkillers\" , \"overstretched\" , \"spiralling downwards\" , \"deliver health and care\"]\n",
        "  output = pd.DataFrame([],columns=['Inglés', 'Español'])\n",
        "  api_root = \"https://linguee-api-v2.herokuapp.com/api/v2\"\n",
        "\n",
        "  for i in range(0, len(list)):\n",
        "    print(i)\n",
        "    resp = requests.get(f\"{api_root}/external_sources\", params={\"query\": i, \"src\": \"en\", \"dst\": \"es\"})\n",
        "    for source in resp.json():\n",
        "      output = output.append({'Inglés': BeautifulSoup(source['src']).text,'Español': BeautifulSoup(source['dst']).text},ignore_index = True)\n",
        "      print(output)\n",
        "    return output\n",
        "    #print(f\"{source['src']} -> {source['dst']}\")\n",
        "\n",
        "def LingueeSearch_ES_EN(data):\n",
        "  search_terms = pd.DataFrame(data, columns= ['Español'])\n",
        "  list = search_terms['Español'].tolist()\n",
        "  print(list)\n",
        "  output = pd.DataFrame([],columns=['Español', 'Inglés'])\n",
        "  api_root = \"https://linguee-api-v2.herokuapp.com/api/v2\"\n",
        "\n",
        "  for i in range(0, len(list)):\n",
        "    resp = requests.get(f\"{api_root}/external_sources\", params={\"query\": i, \"src\": \"es\", \"dst\": \"en\"})\n",
        "    print(i)\n",
        "    for source in resp.json():\n",
        "        output = output.append({'Español': BeautifulSoup(source['src']).text,'Inglés': BeautifulSoup(source['dst']).text},ignore_index = True)\n",
        "        print(output)\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "9MgQvkn_zHyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vWQh6QvMZSJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = \"Flood of departures\"\n",
        "output = LingueeSearch_EN_ES(list)\n",
        "archivo = output.to_excel(r'/content/Strings-enes.xlsx', index = False)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SppUuHutCtts",
        "outputId": "15d9e205-884e-4f9f-f9f8-e889ba0adafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bd26a41c8648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Flood of departures\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLingueeSearch_EN_ES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0marchivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/content/Strings-enes.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-9b33c8d47595>\u001b[0m in \u001b[0;36mLingueeSearch_EN_ES\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLingueeSearch_EN_ES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0msearch_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Inglés'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Inglés'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;31m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TR-EX.ME ###"
      ],
      "metadata": {
        "id": "dpHbWrxg4Gbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TR_EX_enes(data):\n",
        "  search_terms = pd.DataFrame(data, columns= ['Inglés'])\n",
        "  list = search_terms['Inglés'].tolist()\n",
        "  print(list)\n",
        "  #list = [\"Flood of departures\" , \"Cite\" , \"Work-life balance\" , \"Findings\" , \"Department of Health and Social Care\" , \"Engage Britain\" , \"Public conciousness\" , \"Has long had significantly lower numbers\" , \"Around 50 in every 10\" , \"000 staff \" , \"hospital and community health services \" , \"within the next three months\" , \"Analysis of NHS Digital figures found that at least 400 staff a week \" , \"comes alongside evidence\" , \"high turnover among social care workers\" , \"left their roles\" , \"Commons health select committee\" , \"Care work and nursing\" , \"Undervalued professions\" , \"senior occupational therapist\" , \"were named as some of our most undervalued professions\" , \" with 69% saying\" , \"health and social care services\" , \"mental health issues \" , \"preventative healthcare\" , \"It wasn’t a difficult decision to go private\" , \"overworked and undervalued \" , \"recruitment and retention of staff\" , \"under-resourced teams\" , \"massive\" , \"Ultimately\" , \" \" , \"hiatus hernia\" , \"ward for dementia patients\" , \"bedpan\" , \"painkillers\" , \"overstretched\" , \"spiralling downwards\" , \"deliver health and care\"]\n",
        "  output = pd.DataFrame([],columns=['Inglés', 'Español'])\n",
        "  for i in list:\n",
        "    data = {\n",
        "      \"source_text\": i,\n",
        "      \"target_text\": \"\",\n",
        "      \"source_lang\": \"en\",\n",
        "      \"target_lang\": \"es\",\n",
        "      \"npage\": 1,\n",
        "      \"mode\": 0\n",
        "       }\n",
        "    source = \"english\"\n",
        "    target = \"spanish\"\n",
        "    npages = requests.post(\"https://tr-ex.me/translation/\"+source+\"-\"+target+\"/\", headers=headers, data=json.dumps(data)).json()[\"npages\"]\n",
        "    for npage in range(1, 10):\n",
        "        data[\"npage\"] = npage\n",
        "        page = requests.post(\"https://tr-ex.me/translation/\", headers=headers, data=json.dumps(data)).json()[\"list\"]\n",
        "        for word in page:\n",
        "            output = output.append({'Inglés': BeautifulSoup(word[\"s_text\"]).text,'Español': BeautifulSoup(word[\"t_text\"]).text},ignore_index = True)\n",
        "            print(output)\n",
        "  return output"
      ],
      "metadata": {
        "id": "vF5dH0y76Dsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://tr-ex.me/translation/english-spanish/basket+press?search=basket+press\"\n",
        "list = [\"Flood of departures\" , \"Cite\" , \"Work-life balance\" , \"Findings\" , \"Department of Health and Social Care\" , \"Engage Britain\" , \"Public conciousness\" , \"Has long had significantly lower numbers\" , \"Around 50 in every 10\" , \"000 staff \" , \"hospital and community health services \" , \"within the next three months\" , \"Analysis of NHS Digital figures found that at least 400 staff a week \" , \"comes alongside evidence\" , \"high turnover among social care workers\" , \"left their roles\" , \"Commons health select committee\" , \"Care work and nursing\" , \"Undervalued professions\" , \"senior occupational therapist\" , \"were named as some of our most undervalued professions\" , \" with 69% saying\" , \"health and social care services\" , \"mental health issues \" , \"preventative healthcare\" , \"It wasn’t a difficult decision to go private\" , \"overworked and undervalued \" , \"recruitment and retention of staff\" , \"under-resourced teams\" , \"massive\" , \"Ultimately\" , \" \" , \"hiatus hernia\" , \"ward for dementia patients\" , \"bedpan\" , \"painkillers\" , \"overstretched\" , \"spiralling downwards\" , \"deliver health and care\"]\n",
        "output = TR_EX_enes(list)\n",
        "archivo = output.to_excel(r'/content/TR-EX-enes.xlsx', index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cXWYOAl4FAs",
        "outputId": "0cb1f3d2-73a4-4e4b-d328-077e5e51b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Flood of departures', 'Cite', 'Work-life balance', 'Findings', 'Department of Health and Social Care', 'Engage Britain', 'Public conciousness', 'Has long had significantly lower numbers', 'Around 50 in every 10', '000 staff ', 'hospital and community health services ', 'within the next three months', 'Analysis of NHS Digital figures found that at least 400 staff a week ', 'comes alongside evidence', 'high turnover among social care workers', 'left their roles', 'Commons health select committee', 'Care work and nursing', 'Undervalued professions', 'senior occupational therapist', 'were named as some of our most undervalued professions', ' with 69% saying', 'health and social care services', 'mental health issues ', 'preventative healthcare', 'It wasn’t a difficult decision to go private', 'overworked and undervalued ', 'recruitment and retention of staff', 'under-resourced teams', 'massive', 'Ultimately', ' ', 'hiatus hernia', 'ward for dementia patients', 'bedpan', 'painkillers', 'overstretched', 'spiralling downwards', 'deliver health and care']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6f8c8bf70698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tr-ex.me/translation/english-spanish/basket+press?search=basket+press\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Flood of departures\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Cite\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Work-life balance\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Findings\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Department of Health and Social Care\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Engage Britain\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Public conciousness\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Has long had significantly lower numbers\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Around 50 in every 10\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"000 staff \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"hospital and community health services \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"within the next three months\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Analysis of NHS Digital figures found that at least 400 staff a week \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"comes alongside evidence\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"high turnover among social care workers\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"left their roles\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Commons health select committee\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Care work and nursing\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"Undervalued professions\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"senior occupational therapist\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"were named as some of our most undervalued professions\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\" with 69% saying\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"health and social care services\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"mental health issues \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"preventative healthcare\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"It wasn’t a difficult decision to go private\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"overworked and undervalued \"\u001b[0m \u001b...\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTR_EX_enes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0marchivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/content/TR-EX-enes.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-6f33122b1a14>\u001b[0m in \u001b[0;36mTR_EX_enes\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"spanish\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://tr-ex.me/translation/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"npages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"npage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'headers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COLLINS DICTIONARY ###"
      ],
      "metadata": {
        "id": "AkddRL-vZfiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "#def Collins_English_Spanish(word):\n",
        "word = \"hope\"\n",
        "url = 'https://www.collinsdictionary.com/dictionary/english-spanish/'+word\n",
        "#print(url)\n",
        "\n",
        "def parse(self, response):\n",
        "        word = response.request.url.split(\"/\")[-1]\n",
        "        definition_dict = {}\n",
        "\n",
        "        for sections in response.xpath(\"//section[@class='gramb']\"):\n",
        "            try:\n",
        "                part_of_speech = sections.xpath(\".//span[@class='pos']/text()\").extract()[0]\n",
        "            except:\n",
        "                part_of_speech = False\n",
        "            def_list = sections.xpath(\"./ul/li/div[@class='trg']//span[@class='ind']\").extract()\n",
        "            if not def_list:\n",
        "                def_list = sections.xpath(\".//div[@class='empty_sense']//div[@class='crossReference']\").extract()\n",
        "\n",
        "            def_list = [re.sub(r'<.*?>', \"\", i).strip() for i in def_list]\n",
        "            def_list = [i for i in def_list if i]\n",
        "\n",
        "            if def_list and part_of_speech:\n",
        "                if part_of_speech in definition_dict:\n",
        "                    definition_dict[part_of_speech] += def_list\n",
        "                else:\n",
        "                    definition_dict[part_of_speech] = def_list\n",
        "\n",
        "        if definition_dict:\n",
        "            yield {word: definition_dict}\n",
        "        return def_list"
      ],
      "metadata": {
        "id": "V0-AtGdafAjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import scrapy\n",
        "\n",
        "# by Peyman (mohsenikiasari@ce.sharif.edu) in 2019.\n",
        "\n",
        "words = ['I', 'hope', 'you', 'like', 'this', 'dictionary', 'web', 'crawler']\n",
        "\n",
        "#  scrapy crawl oxford -o oxford.jl\n",
        "class OxfordCrawler(scrapy.Spider):\n",
        "    name = \"oxford\"\n",
        "    allowed_domains = [\"www.lexico.com\"]\n",
        "    start_urls = [\"https://www.lexico.com/en/definition/\" + word for word in words]\n",
        "\n",
        "    def parse(self, response):\n",
        "        word = response.request.url.split(\"/\")[-1]\n",
        "        definition_dict = {}\n",
        "\n",
        "        for sections in response.xpath(\"//section[@class='gramb']\"):\n",
        "            try:\n",
        "                part_of_speech = sections.xpath(\".//span[@class='pos']/text()\").extract()[0]\n",
        "            except:\n",
        "                part_of_speech = False\n",
        "            def_list = sections.xpath(\"./ul/li/div[@class='trg']//span[@class='ind']\").extract()\n",
        "            if not def_list:\n",
        "                def_list = sections.xpath(\".//div[@class='empty_sense']//div[@class='crossReference']\").extract()\n",
        "\n",
        "            def_list = [re.sub(r'<.*?>', \"\", i).strip() for i in def_list]\n",
        "            def_list = [i for i in def_list if i]\n",
        "\n",
        "            if def_list and part_of_speech:\n",
        "                if part_of_speech in definition_dict:\n",
        "                    definition_dict[part_of_speech] += def_list\n",
        "                else:\n",
        "                    definition_dict[part_of_speech] = def_list\n",
        "\n",
        "        if definition_dict:\n",
        "            yield {word: definition_dict}\n",
        "\n",
        "\n",
        "#  scrapy crawl longman -o longman.jl\n",
        "class LongmanCrawler(scrapy.Spider):\n",
        "    name = \"longman\"\n",
        "    allowed_domains = [\"https://www.ldoceonline.com\"]\n",
        "    start_urls = [\"https://www.ldoceonline.com/dictionary/\" + word for word in words]\n",
        "\n",
        "    def parse(self, response):\n",
        "        word = response.request.url.split(\"/\")[-1]\n",
        "        definition_dict = {}\n",
        "\n",
        "        for sections in response.xpath(\"//span[@class='dictentry']\"):\n",
        "            try:\n",
        "                part_of_speech = (sections.xpath(\".//span[@class='POS']/text()\").extract()[0]).strip()\n",
        "            except:\n",
        "                part_of_speech = False\n",
        "            def_list = sections.xpath(\".//span[@class='Sense']/span[@class='DEF']\").extract()\n",
        "            def_list = [re.sub(r'<.*?>', \"\", i[18:-7]).strip() for i in def_list]\n",
        "            def_list = [i for i in def_list if i]\n",
        "\n",
        "            if def_list and part_of_speech:\n",
        "                if part_of_speech in definition_dict:\n",
        "                    definition_dict[part_of_speech] += def_list\n",
        "                else:\n",
        "                    definition_dict[part_of_speech] = def_list\n",
        "\n",
        "        if definition_dict:\n",
        "            yield {word: definition_dict}\n",
        "\n",
        "\n",
        "#  scrapy crawl cambridge -o cambridge.jl\n",
        "class CambridgeCrawler(scrapy.Spider):\n",
        "    name = \"cambridge\"\n",
        "    allowed_domains = [\"https://dictionary.cambridge.org\"]\n",
        "    start_urls = [\"https://dictionary.cambridge.org/dictionary/english/\" + word for word in words]\n",
        "\n",
        "    def parse(self, response):\n",
        "        word = response.request.url.split(\"/\")[-1]\n",
        "        definition_dict = {}\n",
        "\n",
        "        for enrty in response.xpath(\"//div[@class='entry-body__el clrd js-share-holder']\"):\n",
        "            part_of_speeches = enrty.xpath(\"./div[@class='pos-header']//span[@class='pos']/text()\").extract()\n",
        "            def_list = enrty.xpath(\n",
        "                \".//div[@class='sense-body']/div[@class='def-block pad-indent']//b[@class='def']\").extract()\n",
        "            def_list = [re.sub(r'<.*?>|:', \"\", i[15:-4]).strip() for i in def_list]\n",
        "            def_list = [i for i in def_list if i]\n",
        "\n",
        "            if def_list and part_of_speech:\n",
        "                for part_of_speech in part_of_speeches:\n",
        "                    if part_of_speech in definition_dict:\n",
        "                        definition_dict[part_of_speech] += def_list\n",
        "                    else:\n",
        "                        definition_dict[part_of_speech] = def_list\n",
        "\n",
        "        if definition_dict:\n",
        "            yield {word: definition_dict}\n",
        "\n",
        "\n",
        "#  scrapy crawl webster -o webster.jl\n",
        "class WebsterCrawler(scrapy.Spider):\n",
        "    name = \"webster\"\n",
        "    allowed_domains = [\"https://www.merriam-webster.com\"]\n",
        "    start_urls = [\"https://www.merriam-webster.com/dictionary/\" + word for word in words]\n",
        "\n",
        "    def parse(self, response):\n",
        "        word = response.request.url.split(\"/\")[-1]\n",
        "        definition_dict = {}\n",
        "\n",
        "        part_of_speeches = [re.sub(r'\\(.*\\)', \"\", i).strip() for i in\n",
        "                            response.xpath(\"//span[@class='fl']/a/text()|//span[@class='fl']/text()\").extract()]\n",
        "\n",
        "        for sections in response.xpath(\"//div[contains(@id, 'dictionary-entry')]/div[@class='vg']\"):\n",
        "            part_of_speech = part_of_speeches.pop(0)\n",
        "            def_list = sections.xpath(\n",
        "                \".//span[@class='dtText' or @class='unText'][not(ancestor::span[@class='dtText'])]\").extract()\n",
        "            def_list = [re.sub(r'<span.*>.+</span>', \"\", i[21:-7]) for i in def_list]\n",
        "            def_list = [re.sub(r'<.*?>|:', \"\", i).strip() for i in def_list]\n",
        "            def_list = [i for i in def_list if i]\n",
        "\n",
        "            if def_list and part_of_speech:\n",
        "                if part_of_speech in definition_dict:\n",
        "                    definition_dict[part_of_speech] += def_list\n",
        "                else:\n",
        "                    definition_dict[part_of_speech] = def_list\n",
        "\n",
        "        if definition_dict:\n",
        "            yield {word: definition_dict}\n",
        "\n",
        "\n",
        "#  scrapy crawl collins -o collins.jl\n",
        "class CollinsCrawler(scrapy.Spider):\n",
        "    name = \"collins\"\n",
        "    allowed_domains = [\"https://www.collinsdictionary.com\"]\n",
        "    start_urls = [\"https://www.collinsdictionary.com/dictionary/english-spanish/\" + word for word in words]\n",
        "\n",
        "    def parse(self, response):\n",
        "        word = response.request.url.split(\"/\")[-1]\n",
        "        definition_dict = {}\n",
        "\n",
        "        for sections in response.xpath(\"//div[@class='dictionary Cob_Adv_Brit']\"\n",
        "                                       \"//div[@class='content definitions cobuild br']/div[@class='hom']\"):\n",
        "            try:\n",
        "                part_of_speech = (sections.xpath(\".//span[@class='pos']/text()\").extract()[0]).strip()\n",
        "            except:\n",
        "                part_of_speech = False\n",
        "            def_list = sections.xpath(\"./div[@class='sense']/div[@class='def']\").extract()\n",
        "            def_list = [re.sub(r'<.*?>', \"\", i[17:-6]).strip() for i in def_list]\n",
        "            def_list = [i for i in def_list if i]\n",
        "\n",
        "            if def_list and part_of_speech:\n",
        "                if part_of_speech in definition_dict:\n",
        "                    definition_dict[part_of_speech] += def_list\n",
        "                else:\n",
        "                    definition_dict[part_of_speech] = def_list\n",
        "\n",
        "        if definition_dict:\n",
        "            yield {word: definition_dict}"
      ],
      "metadata": {
        "id": "JdoJIifWcE3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(example)"
      ],
      "metadata": {
        "id": "mppp1ERKdNA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}